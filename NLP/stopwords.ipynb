{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"Dreams transform into thoughts and thoughts result in action. To succeed in your mission, you must have single-minded devotion to your goal. Small aim is a crime; have great aim. Don't fear failure; failure is the stepping stone to success. The ignited mind of the youth is the most powerful resource on the earth. You have to dream before your dreams can come true. Excellence is a continuous process and not an accident. Don't take rest after your first victory because if you fail in second, more lips are waiting to say that your first victory was just luck. Let us sacrifice our today so that our children can have a better tomorrow. If you want to shine like a sun, first burn like a sun. We should not give up and we should not allow the problem to defeat us. Be active! Take on responsibility! Work for the things you believe in.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOKENIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dreams transform into thoughts and thoughts result in action.',\n",
       " 'To succeed in your mission, you must have single-minded devotion to your goal.',\n",
       " 'Small aim is a crime; have great aim.',\n",
       " \"Don't fear failure; failure is the stepping stone to success.\",\n",
       " 'The ignited mind of the youth is the most powerful resource on the earth.',\n",
       " 'You have to dream before your dreams can come true.',\n",
       " 'Excellence is a continuous process and not an accident.',\n",
       " \"Don't take rest after your first victory because if you fail in second, more lips are waiting to say that your first victory was just luck.\",\n",
       " 'Let us sacrifice our today so that our children can have a better tomorrow.',\n",
       " 'If you want to shine like a sun, first burn like a sun.',\n",
       " 'We should not give up and we should not allow the problem to defeat us.',\n",
       " 'Be active!',\n",
       " 'Take on responsibility!',\n",
       " 'Work for the things you believe in.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "documents = nltk.sent_tokenize(paragraph)\n",
    "snow = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIST OF WORDS IN \"STOPWORDS\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WIHTOUT LEMMATISATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dream transform thought thought result action .',\n",
       " 'to succeed mission , must single-mind devot goal .',\n",
       " 'small aim crime ; great aim .',\n",
       " \"do n't fear failur ; failur step stone success .\",\n",
       " 'the ignit mind youth power resourc earth .',\n",
       " 'you dream dream come true .',\n",
       " 'excel continu process accid .',\n",
       " \"do n't take rest first victori fail second , lip wait say first victori luck .\",\n",
       " 'let us sacrific today children better tomorrow .',\n",
       " 'if want shine like sun , first burn like sun .',\n",
       " 'we give allow problem defeat us .',\n",
       " 'be activ !',\n",
       " 'take respons !',\n",
       " 'work thing believ .']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(documents)):\n",
    "    words = nltk.word_tokenize(documents[i])\n",
    "    words = [snow.stem(word) for word in words if word not in set(\n",
    "        stopwords.words('english'))]\n",
    "    documents[i] = ' '.join(words)\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WITH LEMMATIZATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dream transform think think result action .',\n",
       " 'succeed mission , must single-mind devot goal .',\n",
       " 'small aim crime ; great aim .',\n",
       " \"n't fear failur ; failur step stone success .\",\n",
       " 'ignit mind youth power resourc earth .',\n",
       " 'dream dream come true .',\n",
       " 'excel continu process accid .',\n",
       " \"n't take rest first victori fail second , lip wait say first victori luck .\",\n",
       " 'let us sacrific today children better tomorrow .',\n",
       " 'want shine like sun , first burn like sun .',\n",
       " 'give allow problem defeat us .',\n",
       " 'activ !',\n",
       " 'take respons !',\n",
       " 'work thing believ .']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "for i in range(len(documents)):\n",
    "    words = nltk.word_tokenize(documents[i])\n",
    "    words = [lemma.lemmatize(word, pos='v') for word in words if word not in set(\n",
    "        stopwords.words('english'))]\n",
    "    documents[i] = ' '.join(words)\n",
    "\n",
    "documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
